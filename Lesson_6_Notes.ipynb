{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction \n",
    "\n",
    "CNNs pass an input image through filters, which extract certain features from the input image.\n",
    "\n",
    "This process of extracting features is useful for more than just image classification, but also for creating and changing images! \n",
    "\n",
    "Style transfer is the process of applying the style of one image to another content image, such that the content of one image is merged with the style of another image. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Style Transfer \n",
    "\n",
    "Adding the style of one image to the content of another can be done with help of max pooling layers from a pretrained network. \n",
    "\n",
    "* **Max pooling layers** eliminate unuseful information, such that only the content representation is left (ie. edges) \n",
    "* **Feature Space**: Contains of texture and color information, used to represent the style of the images. \n",
    "* **Feature maps**: The output after applying a filter to an image. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting features\n",
    "\n",
    "Using the `vgg19` network, the content and the style image will be passed through the vgg network. \n",
    "\n",
    "**Getting the features** \n",
    "1. Feedforward content image until getting to a deep convolutional layer. \n",
    "2. The output of the layer is the content representation of the input image \n",
    "3. Send the style image into the network \n",
    "4. Extract different features from multiple layers which represent the style of the image. \n",
    "\n",
    "\n",
    "The next step is to to create a target image, but how is this done? \n",
    "\n",
    "1. During forming the new image, compare the content with the original image's content using a content loss \n",
    "2. A content loss calculates the difference between the content and target image representation. Content: Cc Target: Tc. The aim is to minize the loss for the images to have the same content.\n",
    "3. Finally, calculate the mean squared difference between the two losses. \n",
    "\n",
    "### Note that this is not the whole picture yet and there is still a style loss to define"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gram Matrix \n",
    "\n",
    "The style representation of an image is based on correlations between features in individual layers. \n",
    "\n",
    "Multi-scale style representation captures different sizes of style features. \n",
    "\n",
    "A gram matrix is created by flatenning feature maps into 2D matrix of values, and multiplying the matrix by the transpoe(the matrix flipped) \n",
    "\n",
    "The gram matrix represents the similarties between the matrix and it's flipped version, which represents how close the style of the original image is to the old image. \n",
    "\n",
    "**Quiz**: When the height and the width are flattenned, the new matrix will have height X width dimensions. \n",
    "\n",
    "**Quiz 2**: The gram matrix will have a width and height equal to the depth of the network. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The style loss is calculated by finding the mean squared difference between the gram matrices of the style and the target images. \n",
    "\n",
    "Note that the style and content loss are calculated differently they will be different but the target images has to have an about equal amount of each. In order to deal with, constants alpha and beta are applied to the style and content.\n",
    "\n",
    "The style must be multiplied by a much large constant in order for the image to change it's style. This is because if the style ratio is small, the images would appear like the **content** image "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters Summary\n",
    "\n",
    "1. Which content layers to use for content calculation - note that this doesn't make as much difference as choosing style layers \n",
    "2. Which style layers to use for style calculation  - in the lesson this was done using the research paper, but different layers can be used! \n",
    "3. Content weight (constant, multiply with the content loss)\n",
    "4. Style weight (constant, multiply with the style loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coding notes \n",
    "\n",
    "When loading the neural network, only the convlutional and pooling layers need to be loaded in, this is done by the following code, not that this will not work for a different type of network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg = models.vgg19(pretrained=True).features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Excercise 1**: In order to choose the layers you must create a dictionary that takes the required content and style layers. It is neccessary that the numbers are the same as they are in the vgg network.\n",
    "\n",
    "**Excercise 2**: The gram matrix function takes the output of a convolutional layer, grabs it's shapes and reshapes it to be 2 dimensional. Finally, it multiplies the matrix with the inverted matrix as seen in the lesson. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, d, h, w = tensor.size()\n",
    "tensor = tensor.view(d, h * w)  # converting to 2-D\n",
    "gram = torch.mm(tensor, tensor.t()) # tensor.t gives us the inverted matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:anaconda3]",
   "language": "python",
   "name": "conda-env-anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
