{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson - 1: Introduction to Neural Networks\n",
    "\n",
    "\n",
    "\n",
    "**Neural Networks**: \n",
    "Neural networks are composed of neurons, which recieve input, process it and generate output. Each of this neurons has an assigned weight which is first assigned randomly but is than changed after training. In addition to this weights, there are biases, which are added to the result of matrix multiplications and is used to change the range of the weight multiplied input. Once this linear components are applied to the input, a non-linear function is applied which translates input signals \n",
    "\n",
    "1. Programs that determine a line to classify data into different types.\n",
    "2. With an increasing complex data a more complex network is required. \n",
    "3. Look at the previous data and use it to make predictions. \n",
    "\n",
    "\n",
    "**Feedforward**: The process neural networks use to turn input into output.\n",
    "\n",
    "**Classification**: Looking for the best line to model a certain data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "**Example #1**: \n",
    "1. Let test be x1 and grades be x2 and let the boundary line be: 2x1 + x2 - 18 = 0\n",
    "2. The `boundary line` is simply the formula to compute whether a given student passed or failed. \n",
    "3. Anything above 0 is considered a `pass` where anything below 0 is `failed`. \n",
    "\n",
    "Note: Don't worry about how the line was created, this will be learnt later in the course \n",
    "\n",
    "\n",
    "Generally, the boundary will be an equation of the form `w1x1 + w2x2 + b = 0`\n",
    "\n",
    "This can be abbreviated as wx + b = 0 given that:\n",
    "1. W = (w1, w2)\n",
    "2. x = (x1, x2)\n",
    "\n",
    "\n",
    "\n",
    "**Label(y)**: What the neural netowork is trying to predict. For example, when classifying cats vs. dogs the labels are cats and dogs. In our example:\n",
    "1. y = 0 means the label is blue \n",
    "2. y = 1 means the label is red \n",
    "\n",
    "**Prediction(y-hat)**: The neural network prediction. \n",
    "    * 1 if wx + b >= 0\n",
    "    * 0 if wx + b < 0\n",
    "\n",
    "In 3 dimensions, the only change is that instead of a line we will have a plane in 3d. For example: w1x1 + w2x2 + w3x3 + b = 0 is still `wx + b = 0`, simply with different dimensions. So even with infinity dimensions, the equation for a boundary will be in the form `wx + b = 0`. \n",
    "\n",
    "**Perception**: \n",
    "\n",
    "An algortithm used to change the weights \n",
    "\n",
    "Check out this link that shows how weights are changed: http://prntscr.com/ltccv7\n",
    "\n",
    "**Pseudo Code:**\n",
    "1. Iterate over all the data points\n",
    "2. Calculate the prediction\n",
    "3. If the prediction is zero, increase the weights by the learning rate\n",
    "4. If the prediction is one, decrease the weights by the learning rate \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perceptronStep(X, y, W, b, learn_rate = 0.01):\n",
    "    for i in range(len(X)):\n",
    "        y_hat = prediction(X[i],W,b)\n",
    "        if y[i]-y_hat == 1:\n",
    "            W[0] += X[i][0]*learn_rate\n",
    "            W[1] += X[i][1]*learn_rate\n",
    "            b += learn_rate\n",
    "        elif y[i]-y_hat == -1:\n",
    "            W[0] -= X[i][0]*learn_rate\n",
    "            W[1] -= X[i][1]*learn_rate\n",
    "            b -= learn_rate\n",
    "    return W, b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**AND operators**: Takes two inputs and returns an output. This can also be represented as a line, where negative area returns a 0 and positive area returns 1. In this perception, we need the bias to be opposite from the weights and slightly greater in magnitude but not greater than the sum of the two weights.\n",
    "\n",
    "**Perception trick**: Start random, look at how badly the line is doing and move it around to get it to work better and better. We look at each point and get information from them to improve the line.\n",
    "\n",
    "**Mathematical Trick**: We subtract the point value from the weights which we multiply by the learning rate.Or we add them if the point is in the other area. \n",
    "\n",
    "**Error**: Looking for small steps to descent and find a local minimum. We should use continuous function to know how much in percentage. In order to do this we will use sigmoid function which gives us number close to 1 and numbers close to 0, or 0.5.  This kind of function will also provide us with a percentage output instead of a number output, allows us to maybe use multiple neural networks. \n",
    "\n",
    "**Sigmoid**: The continuous version of the step function, where there are small numbers instead of discrete numbers. For large numbers: close to 1, small: close to 0, 0: 0.5\n",
    "\n",
    "**Softmax function**: Used in the output layer for classification problems. Softmax is similiar to sigmoid except that the outputs are normalized to sum up to 1. Softmax makes it really easy to assign values to each class.  \n",
    "\n",
    "**One hot encoding**: The process of instead of having one variable we can have n variables with one criteria where each represents either an 1 or a 0.\n",
    "\n",
    "**Cross Entropy**: Using the logarithm function to determine whether a mode is good or bad. Goal: minimize the cross entropy, meaning the sum of the log of all the points. What’s interesting is that the cross entropy uses probability to generate an error function using probability.\n",
    "\n",
    "**Cross Entropy algorithm**: the sum of yi * log(pi) + (1-yi) * log(1 - pi)\n",
    "\n",
    "**Multi-Class**: the negative of the summation of i_equals_one to n up to the summation from y_equals_j to m Yij * log(Pij). YIJ = 1 makes sure that we are only adding the logarithm of the probabilities of the events that actually have occured.\n",
    "* M: number of classes\n",
    "\n",
    "**Logistic Regression algorithm**: Pick a random model for the data, calculate the error(cross entropy?), minimize the error and continue doing that.\n",
    "\n",
    "Pseudo code:\n",
    "\n",
    "* If y = 1:\n",
    "* P(blue) = yhat\n",
    "* Error = -ln(yhat)\n",
    "* If y = 0:\n",
    "* P(red) = 1 - P(blue) = 1 - yhat\n",
    "* Error = -ln(1-yhat)\n",
    "* Error = -(1-y)(ln(1-yhat)) - yln(yhat)\n",
    "* Error function: The sum over all the error functions\n",
    "\n",
    "When we use the error function, we can move from E(W, b) to something like E prime w prime b, and since prime is a local minimum this is nice for us.\n",
    "\n",
    "**Gradient Descent**: Optimization algorithm, the goal is to minimize the error function. Gradient Descent is like climbing down a hill, you should take small steps and walk down slowly. \n",
    "\n",
    "\n",
    "Pseudo code:\n",
    "* Start with random weights, w1...wn, b\n",
    "* For every point(x1….xn):\n",
    "* For i = 1….n\n",
    "* Update wi’ ← wi - a(y^-y)xi\n",
    "* Update b’ ← b - a(y^-y)\n",
    "* Repeat until the error is small.\n",
    "\n",
    "Doing arithmetic on model: A linear model gives us the probability for each point to be blue. We can combine the two by adding. We use the sigmoid function to get a new value, the probability. We can also have one model weight more by adding weights. Than we add the two probabilities. Than we apply the sigmoid function. \n",
    "We can take a linear combination of two models so the two models time a constant will give us a new model.\n",
    "\n",
    "**Feedforward**: The process neural networks to move input from the hidden layers to the output layers. The information always travelled forward. \n",
    "\n",
    "**Backpropagation**  the process of training neural networks, consists of:\n",
    "\n",
    "1. Doing feedforward operation\n",
    "2. Comparing the output of the model with the desired output(target)\n",
    "3. Calculating the error \n",
    "4. Running the feedforward operation backwards to spread the error to each of the weights\n",
    "5. Use this to update the weights and get a better model\n",
    "\n",
    "In backpropagation we ask the point what it wants the region to do so that the model will classify it correctly, than we look over the two models and check which one is classifying the point and better and we listen to the model more, by increasing its weight. \n",
    "With backpropagation we ask each point what it wants the models to do in order for them to classify it correctly.\n",
    "Gradient: The vector formed by all the partial derivatives of the error function with respect to the weight wi up to wn. \n",
    " \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Lab: Gradient Descent\n",
    "Functions to implement:\n",
    "Sigmoid activation function\n",
    "Output_formula: The formula for the prediction\n",
    "Error_formula: The formula for the error at a point\n",
    "Update_weights: The function that updates the parameters with one gradient descent step.\n",
    "\n",
    "\n",
    "**Conclusion**: in this lab four function were implemented for the gradient descent algorithm. \n",
    "\n",
    "1. **sigmoid function**: Used to change to make the value of the neural network output continues instead of discrete. Equation: 1/ (1 + e-x). \n",
    "2. **Prediction formula**: Taking the dot product of the features and the weights and finally adding the bias.\n",
    "3. **Error formula** Used to tell how badly is the output function doing.\n",
    "4. **Update weights function**: using the output formula to determine the error -(y - y^). Than decrease the weights by the learn rate times the point times the error and decrease the bias by the error times the learn-rate. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**Dropout**: Dropout is a regulazation technique to avoid overfitting. Dropout is useful so that all the weights have an equal share of their affect on the overall result, rather than one weight taking over.  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "1. Error & Loss Functions:\n",
    "    - Maximum likelihood\n",
    "    - Log Loss\n",
    "    - CrossEntropy\n",
    "2. Layers:\n",
    "    - Input\n",
    "    - Hidden layers(can be many) \n",
    "    - Output\n",
    "3. Optimization functions\n",
    "    - Gradient Descent\n",
    "    - SGD\n",
    "    - RMSProp\n",
    "    - Momentum\n",
    "    - Adagrad\n",
    "    - Adadelta\n",
    "\n",
    "4. Training Optiminzation:\n",
    "    - Learning rate decay \n",
    "    - Regulazations techniques:\n",
    "        - L1\n",
    "        - L2\n",
    "        - Early Stopping\n",
    "        - dropout\n",
    "    - Local Minima\n",
    "        - Random Restarts\n",
    "        - Momentum\n",
    "    - Vanishing Gradients\n",
    "        - Use different activation functions, ie relu\n",
    "        - Stochastic Gradient Descent\n",
    " \n",
    "5. Activation Functions\n",
    "    * Sigmoid\n",
    "    * ReLU\n",
    "    * tanh\n",
    "    * softmax\n",
    "    * more actiovation functions can be found here: https://pytorch.org/docs/stable/nn.html\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:anaconda3]",
   "language": "python",
   "name": "conda-env-anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
