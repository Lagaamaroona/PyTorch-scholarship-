{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 5 notes: convelutional neural networks\n",
    "\n",
    "Convelutional neural networks are a different type of networks, useful for:\n",
    "1. Voice User Interface: Wavenet\n",
    "2. Natural Language Processing\n",
    "3. Computer Vision \n",
    "\n",
    "\n",
    "\n",
    "## Features: \n",
    "The shapes and colors that define an image. Part of the job of a CNN is to identify the features in order to detect patterns in an image, such as edges.\n",
    "\n",
    "For example, in the case of a dog image the features can be the size, shape, legs, and so on. \n",
    "\n",
    "What makes up an image is the **features** of an image "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How Computers Interpret Data\n",
    "\n",
    "1. **Grayscale image:** a grid of values, maid up of pixels with numerical values. \n",
    "2. **Pre-process data:** Making each pixel between 0 and 1 instead of 0 to 255 (normalization). This helps the algorithm work better when detecting features. \n",
    "3. **MLP**: Multi-layer Perceptron, the method learned in the previous lesson takes vector as input such that a 28 by 28 images will simply be a vector of 784 units. \n",
    "4. **Flattening**: The process of converting an image to a vector, row 1 is first part, row 2 is second part, etc..\n",
    "\n",
    "\n",
    "Keep in mind that although flattenning is used in MLP, this strategy is not perfect because the network doesn't learn based on which pixels are next to each and with real world data an MLP is not the best choice.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Layer Perceptions\n",
    "\n",
    "Multi-Layer Perceptions(MLP) are the networks we have been construcing during lesson 4. This networks consisted of fully connected layers starting from input layers to hidden layers and finally to an output layers. \n",
    "\n",
    "The task of classifying image in an MLP is a multi-process step. \n",
    "1. First, Visualize the data set to understand input size and the task. \n",
    "2. Normalize data, applying any transformations required such as converting to a tensor. \n",
    "3. Define a model and a pre-trained network. \n",
    "4. Train the model\n",
    "5. Test the model \n",
    "\n",
    "Next, it's time to start working convulutional neural networks which are another way of working with images. \n",
    "\n",
    "**Important Definitions**:\n",
    "1. **Class Score**: The output of the network, Indicates how sure a network is that a given input is of a specific task. \n",
    "2. **Loss**: Measure any difference from a predicted and a true class\n",
    "3. **Backrpopagation**: Quatify how bad a particular weight is in making a mistake\n",
    "4. **Optimization**: Gives us a way to calculate a better weight value\n",
    "5. **Cross-Entropy Loss**: Looks at the label's probability value and takes the negative Loss log of that value. The loss is lower when the loss and prediction agree. \n",
    "6. **Types of optimizers**: See code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Note that the best optimizers are SGD and Adam because they are combinations of the other optimizers'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch import optim\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(784, 256)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.fc3 = nn.Linear(128, 64)\n",
    "        self.fc4 = nn.Linear(64, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # make sure input tensor is flattened\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        \n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = F.log_softmax(self.fc4(x), dim=1)\n",
    "        \n",
    "        return x    \n",
    "model = Classifier()\n",
    "\n",
    "# Optimizers: \n",
    "adam = optim.Adagrad(model.parameters(), lr=0.001)\n",
    "adadelta = optim.Adadelta(model.parameters(), lr=0.001)\n",
    "SGD = optim.SGD(model.parameters(), lr=0.001)\n",
    "rmsProp = optim.RMSprop(model.parameters(), lr=0.001)\n",
    "Adadelta = optim.Adadelta(model.parameters(), lr=0.001)\n",
    "\n",
    "\"\"\" Note that the best optimizers are SGD and Adam because they are combinations of the other optimizers\"\"\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Neural Networks \n",
    "\n",
    "\n",
    "Convolutional Neural Networks are a different types of network, much better for real-world data where the actual image may be anywhere around the large image. Some reason of this include:\n",
    "\n",
    "1. MLPs are flatten the data, converting it into a vector. Due to this, MLPS and are not aware what the initial image looks like and they can't understand the relationship between a pixel and the pixel above it. \n",
    "2. In comparison, CNN understand that pixels closer to each other are more related by accepting matrices as input. \n",
    "3. MLPs only use fully connected layers, where CNNs use sparsely connected layers. This means that the CNN doesn't immediately look at whole image, but it looks at parts of the image in order to make up the bigger picture. \n",
    "4. In a CNN every hidden layer recieves a part of the data the previous layer had. \n",
    "\n",
    "\n",
    "Basically, in any case where data is laid in a more complciated arichtecture such that the data can be anywhere in the images instead of simply in the center, a CNN has a serious advantage. \n",
    "\n",
    "\n",
    "## Filters\n",
    "\n",
    "The job of filters in the convolutional layer is to extract information from the image such as the edges. **Intensity** is the measure of brightness, intensity can be used to detect the shape for tasks like distinguishing boundries between people and background. \n",
    "\n",
    "**Frequency** is the rate of change whhere high frequency means a high change in colors and low frequency is simply a low or no change. For example, in an image of person with white background the person outline would have high frequency. \n",
    "\n",
    "**High-Pass Filter** is the process of emphasizing edges and darking out areas where the roc is low and changing images where the roc is high to white. \n",
    "\n",
    "**Convolution Kernels**: Taking a kernel and passing it through an image\n",
    "\n",
    "The process of using a high pass filter involves taking a kernel, and multiplying it by each set of pixels in the images. A high value indicates an edge. However, what happens when you get to the edges of the image? "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Layer\n",
    "\n",
    "* A stack of feature maps - one feature map for each filter \n",
    "* Produced by applying a series of different images filters, aka convolutionals kernels. \n",
    "* The amount of kernels equals the amount of images produced \n",
    "\n",
    "\n",
    "**Hyperparameters:**\n",
    "\n",
    "* Increasing the number of filters increases the number of nodes.\n",
    "* Increase size of each filter increases the size of the patterns\n",
    "* Stride: The amount by which the filter slides over the window.\n",
    "\n",
    "\n",
    "One possible issue with the convolutional layer is dealing with the nodes outside of the image. Solutions include:\n",
    "1. Ignoring them - this might cause an issue of the edges being undefined\n",
    "2. Pad the image with 0 to give the filter more space to move. \n",
    "\n",
    "\n",
    "**Define a convolutional layer:**\n",
    "\n",
    "1. Define the layers of the model in the `__init__` function\n",
    "2. Use `super(Net, self).__init__()` just like in an MLP \n",
    "3. Initialize the height and width based on the filter value\n",
    "4. Set self.conv `nn.Conv(1, gray_scalefilters, kernel_size, bias)` \n",
    "5. Set `self.conv.weight` to be `torch.nn.Parameters(weight)`\n",
    "6. In the forward function, calculate the output of the convolutional layer using `self.conv(x)` \n",
    "7. Run the output through a relu activation \n",
    "9. Return the output, and the value before activation. \n",
    "\n",
    "**Example:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "    \n",
    "# define a neural network with a single convolutional layer with four filters\n",
    "class Net(nn.Module):\n",
    "    \n",
    "    def __init__(self, weight):\n",
    "        super(Net, self).__init__()\n",
    "        # initializes the weights of the convolutional layer to be the weights of the 4 defined filters\n",
    "        k_height, k_width = weight.shape[2:]\n",
    "        # assumes there are 4 grayscale filters\n",
    "        self.conv = nn.Conv2d(1, 4, kernel_size=(k_height, k_width), bias=False)\n",
    "        self.conv.weight = torch.nn.Parameter(weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # calculates the output of a convolutional layer\n",
    "        # pre- and post-activation\n",
    "        conv_x = self.conv(x)\n",
    "        activated_x = F.relu(conv_x)\n",
    "        \n",
    "        # returns both layers\n",
    "        return conv_x, activated_x\n",
    "    \n",
    "# instantiate the model and set the weights\n",
    "weight = torch.from_numpy(filters).unsqueeze(1).type(torch.FloatTensor)\n",
    "model = Net(weight)\n",
    "\n",
    "# print out the layer in the network\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pooling Layers\n",
    "\n",
    "* While training using CNN, a large number of filters may be required for this large data sets.\n",
    "* Pooling layers reduce the extra dimensions created by the convolutional layer \n",
    "* Max pooling layer: Given a stack of feature maps returns the greatest value in each feature map\n",
    "    * Define a window size and stride (eg. Window 2X2, Stride: 2)\n",
    "    * Start with the top left corner window \n",
    "    * Take the maximum value in the window \n",
    "    * Continue for all features \n",
    "    * Finally, the final width and height are half of the previous convolutional layer. \n",
    "    \n",
    "\n",
    "* Average pooling layer: Chooses an average pixel values in a given window size \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Increasing Depth\n",
    "\n",
    "1. Using convolutional layers to make the array deeper\n",
    "2. Maxpooling layers will be used to decrease the xy dimension "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other Notes\n",
    "\n",
    "1. Lesson 5.35: To show an image, it is important to first convert the image into a numpy image.  \n",
    "1. When defining a network, don't forget to include the linear transformations. \n",
    "* [Wavenet model](https://deepmind.com/blog/wavenet-generative-model-raw-audio/): A CNN application, taking wavefroms recorded from humans used to generate an AI similiar to humans."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:anaconda3]",
   "language": "python",
   "name": "conda-env-anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
