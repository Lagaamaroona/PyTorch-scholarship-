{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson 4 Notes\n",
    "\n",
    "1. Pytorch is the main data structure in PyTorch\n",
    "2. Autograd is a model used to calculuate gradients for training neural networks "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single Layer Neural Network \n",
    "\n",
    "\n",
    "1. Simple Example: given various input values multiply by the weights, sum all of them up to achieve the output. Than pass this through an activation function. \n",
    "2. The output can be generated by finding the dot product of the input and the weights. \n",
    "3. A sigmoid activation function squizzes the values between 0 and 1. \n",
    "4. torch.rand(x): Creates a random tensor. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.8699, 0.7125, 0.8014])\n",
      "tensor([0.1246, 0.5284, 0.1289])\n",
      "tensor([0.5553, 0.0534, 0.9205])\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "def sigmoid(x):\n",
    "    return 1/(1+torch.exp(-x))\n",
    "\n",
    "for i in range(3):\n",
    "    x = torch.rand(3)\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Networks \n",
    "\n",
    "1. Multiplying the features by weights returns a new tensor. \n",
    "2. Multiple ways to change a tensor. \n",
    "\n",
    "---2.1: Reshape, the tensor must be the same size\n",
    "\n",
    "---2.2: Resize: The shape changes\n",
    "\n",
    "---2.3: View: Returns a new tensor with the same size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Method 1: Reshaping\n",
      "tensor([[-0.3364],\n",
      "        [-0.3000],\n",
      "        [ 0.9367],\n",
      "        [-0.8321],\n",
      "        [-0.8589]])\n",
      "Method 2: Resizing\n",
      "tensor([[-0.3364, -0.3000,  0.9367],\n",
      "        [-0.8321, -0.8589,  0.0000]])\n",
      "Method 3: View\n",
      "tensor([[-0.3364, -0.3000,  0.9367, -0.8321, -0.8589,  0.0000]])\n"
     ]
    }
   ],
   "source": [
    "features = torch.rand((1,5))\n",
    "weights = torch.randn_like(features)\n",
    "\n",
    "print(\"Method 1: Reshaping\")\n",
    "weights = weights.reshape(5, 1)\n",
    "print(weights)\n",
    "\n",
    "print(\"Method 2: Resizing\")   \n",
    "weights.resize_(2, 3)\n",
    "print(weights)\n",
    "\n",
    "print(\"Method 3: View\")\n",
    "weights = weights.view(1, 6)\n",
    "print(weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Player Network \n",
    "\n",
    "1. The output of the first layer becomes the input of the next layer \n",
    "2. It is also possible to convert between numpy and pyTorch arrays "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.17698887 0.43140258 0.44533437 0.81781291 0.00185514 0.98560505]\n",
      " [0.7348304  0.69331036 0.295731   0.45153352 0.6511645  0.75648109]\n",
      " [0.72901256 0.51150662 0.65010497 0.75495484 0.98788565 0.55775569]\n",
      " [0.02354275 0.20867556 0.09601424 0.06477305 0.21147404 0.32829829]\n",
      " [0.9228853  0.18409156 0.70917809 0.02389355 0.41605309 0.53638009]]\n",
      "tensor([[0.1770, 0.4314, 0.4453, 0.8178, 0.0019, 0.9856],\n",
      "        [0.7348, 0.6933, 0.2957, 0.4515, 0.6512, 0.7565],\n",
      "        [0.7290, 0.5115, 0.6501, 0.7550, 0.9879, 0.5578],\n",
      "        [0.0235, 0.2087, 0.0960, 0.0648, 0.2115, 0.3283],\n",
      "        [0.9229, 0.1841, 0.7092, 0.0239, 0.4161, 0.5364]],\n",
      "       dtype=torch.float64)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.17698887, 0.43140258, 0.44533437, 0.81781291, 0.00185514,\n",
       "        0.98560505],\n",
       "       [0.7348304 , 0.69331036, 0.295731  , 0.45153352, 0.6511645 ,\n",
       "        0.75648109],\n",
       "       [0.72901256, 0.51150662, 0.65010497, 0.75495484, 0.98788565,\n",
       "        0.55775569],\n",
       "       [0.02354275, 0.20867556, 0.09601424, 0.06477305, 0.21147404,\n",
       "        0.32829829],\n",
       "       [0.9228853 , 0.18409156, 0.70917809, 0.02389355, 0.41605309,\n",
       "        0.53638009]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "x = np.random.rand(5, 6)\n",
    "print(x)\n",
    "x = torch.from_numpy(x)\n",
    "print(x)\n",
    "x.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Networks\n",
    "\n",
    "1. inline: Allows to build large neural networks in an effecient way. \n",
    "2. Pytorch's nn module provides a nice way to build large neural networks \n",
    "3. Torchvision: A package along pytorch that provides datasets and models. Can be used for:\n",
    "\n",
    "3.1. datasets\n",
    "\n",
    "3.2. transforms\n",
    "\n",
    "3.3. models\n",
    "\n",
    "4. Batch size: The amount of images per batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 4 Axes>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 250,
       "width": 317
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "import time \n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                               transforms.Normalize((0.5,0.5,0.5), (0.5,0.5,0.5))\n",
    "                               ])\n",
    "\n",
    "trainset = datasets.MNIST('MNIST_data/', download=True, train=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)   # 64 images per trainloader \n",
    "\n",
    "\n",
    "dataiter = iter(trainloader)\n",
    "images, labels = dataiter.next()\n",
    "\n",
    "f, axarr = plt.subplots(2, 2);\n",
    "axarr[0,0].imshow(images[4].numpy().squeeze(), cmap='Greys_r');\n",
    "axarr[0,1].imshow(images[0].numpy().squeeze(), cmap='Greys_r');\n",
    "axarr[1,0].imshow(images[1].numpy().squeeze(), cmap='Greys_r');\n",
    "axarr[1,1].imshow(images[2].numpy().squeeze(), cmap='Greys_r');\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax \n",
    "\n",
    "To calcluate the probability distribution we use a softmax function. Mathamtical appearance:\n",
    "$$\n",
    "\\Large \\sigma(x_i) = \\cfrac{e^{x_i}}{\\sum_k^K{e^{x_k}}}\n",
    "$$\n",
    "Note that this function will only return a value between 0 and 1\n",
    "\n",
    "1. Create a tensor that is a vector of 64 elements by adding all the exponents \n",
    "2. Resize the tensor\n",
    "2. Divide by the exponential valaue "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    \"\"\"Takes any range of real numbers and returns the output value. \n",
    "    This function produces a curve in shape \"S\"\n",
    "    Used for binary classification \"\"\"\n",
    "    return 1/(1+torch.exp(-x))\n",
    "\n",
    "def softmax(x):\n",
    "    total = torch.sum(torch.exp(x), dim=1).view(-1,1)\n",
    "    expL = torch.exp(x)\n",
    "    return total/expL\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building networks with PyTorch\n",
    "\n",
    "1. We new to use .view() to flatten the tensor into a useful shape. For example: `images.view(images.shape[0], -1)` \n",
    "2. No matter which method is chosen, input units are multiplied by weights, hidden units are multiplied by weights and so until the output is reached\n",
    "\n",
    "**Method 1: Class**\n",
    "\n",
    "Long Way: \n",
    "1. Must use `super().__init__()` so PyTorch registers all the all the operations you put in the network.\n",
    "2. Insinde init use: `nn.Linear(input, hidden)` to create a linear transformation\n",
    "3. Inside init use: `self.sigmoid = nn.Sigmoid()` to define a sigmoid function using the class\n",
    "4. Inside init use: `self.softmax = nn.Softmax(dim=1)` to define a softmax function\n",
    "5. Create `forward()` method which represents the tensor going through the operations. \n",
    "6. Inside forward use: `x = self.hidden(x)` passing the tensor through the hidden layer\n",
    "7. Inside forward use: `x = self.sigmoid(x)` which runs through sigmoid activation\n",
    "8. Inside forward use: `x = self.output(x)` passing the tensor through the output layer\n",
    "9. Inside forward use: `x = self.softmax(x)` which runs through softmax activation\n",
    "\n",
    "Shorter Way:\n",
    "Import `torch.nn.functional as F` which makes the code nicer so we don't have to actually define sigmoid and softmax inside init and instead they are included in the forward method. This is because sigmoid and softmax are simply element wise operations so there is no need to define extra elements. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()    # using the init model of nn.Module \n",
    "        \n",
    "        input_size = 784 \n",
    "        hidden_size = [128, 64]\n",
    "        output_size = 10\n",
    "        self.hidden1 = nn.Linear(input_size, hidden_size[0])\n",
    "                                 \n",
    "        self.hidden2 = nn.Linear(hidden_size[0], hidden_size[1])  \n",
    "        # Output layer, 10 units - one for each digit\n",
    "        self.output = nn.Linear(hidden_size[1], output_size)   \n",
    "        \n",
    "        # Define sigmoid activation and softmax output \n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "    def forward(self, x):   \n",
    "        # Pass the input tensor through each of our operations and eventually give us an output. \n",
    "        x = self.hidden1(x)  # passing through hidden \n",
    "        x = F.relu(x)\n",
    "        s = self.hidden2(x)\n",
    "        x = F.relu()\n",
    "        x = self.output(x)\n",
    "        x = F.softmax(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation Functions \n",
    "\n",
    "There are multiple activation functions available\n",
    "\n",
    "1. Sigmoid\n",
    "2. Relu: Rectified linear unit.  \n",
    "3. TanH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Neural Networks \n",
    "\n",
    "1. Universal Function approximator: Networks that approximate a function to model a dataset\n",
    "2. Loss Function: Depends on the output of the network \n",
    "3. Gradient: The slope of a loss function, points in direction of the fastest change\n",
    "4. Backpropgation: Taking the gradient and passing them backwards to find the new weights\n",
    "5. Learning Rate: A way to avoid bouncing too far\n",
    "\n",
    "** Different Types Of Losses: ** \n",
    "\n",
    "1. `nn.CrossEntropyLoss`\n",
    "2. `nn.NLLLoss()`\n",
    "\n",
    "** Grad **\n",
    "1. `requires_grad = True` tells PyTorch to track the operations on a tensor.\n",
    "2. `torch.set_grad_enables(True|False)` allow you to set gradients globally\n",
    "\n",
    "** Actual Training **\n",
    "\n",
    "1. Define amount of epochs \n",
    "2. For the range of epochs (set running_loss = 0)\n",
    "3. for images, labels in trainloader:\n",
    "4. `optimizer.zero_grad()`\n",
    "5. Pass the images through the model\n",
    "6. Calclulate the loss\n",
    "7. Do a backward pass\n",
    "8. Do the optimzier step\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(nn.Linear(784, 128),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(128, 64),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(64, 10),\n",
    "                      nn.LogSoftmax(dim=1))\n",
    "\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.003)\n",
    "\n",
    "epochs = 5\n",
    "for e in range(epochs):\n",
    "    running_loss = 0\n",
    "    for images, labels in trainloader:\n",
    "        # Flatten MNIST images into a 784 long vector\n",
    "        images = images.view(images.shape[0], -1)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = model.forward(images)\n",
    "        loss = criterion(output, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    else:\n",
    "        print(f\"Training loss: {running_loss/len(trainloader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference and Validation\n",
    "\n",
    "1. Inference: Using your trained network to make predictions\n",
    "2. `ps.topk()`: Returns the probability value and the class for the highest probability values. \n",
    "\n",
    "** Process**\n",
    "\n",
    "1. After every epoch, Initialize validation variables\n",
    "2. for images, labels in testloader:\n",
    "3. Run the images through the model \n",
    "4. Measure the test_loss\n",
    "5. Check whether the top class equals the output \n",
    "6. Increase the accuracy if it equals given that equals is a FloatTensor\n",
    "7. Append the running_loss divided the length of the trainloader to train_losses\n",
    "8. Repeat for test losses\n",
    "9. Print our accuracy, loss, Train Loss, and epoch. \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Classifier()\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.003)\n",
    "\n",
    "epochs = 5\n",
    "steps = 0\n",
    "\n",
    "train_losses, test_losses = [], []\n",
    "for e in range(epochs):\n",
    "    running_loss = 0\n",
    "    for images, labels in trainloader:\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        log_ps = model(images)\n",
    "        loss = criterion(log_ps, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        \n",
    "    else:\n",
    "        test_loss = 0\n",
    "        accuracy = 0\n",
    "        \n",
    "        # Turn off gradients for validation, saves memory and computations\n",
    "        with torch.no_grad():\n",
    "            for images, labels in testloader:\n",
    "                log_ps = model(images)\n",
    "                test_loss += criterion(log_ps, labels)\n",
    "                \n",
    "                ps = torch.exp(log_ps)\n",
    "                top_p, top_class = ps.topk(1, dim=1)\n",
    "                equals = top_class == labels.view(*top_class.shape)\n",
    "                accuracy += torch.mean(equals.type(torch.FloatTensor))\n",
    "                \n",
    "        train_losses.append(running_loss/len(trainloader))\n",
    "        test_losses.append(test_loss/len(testloader))\n",
    "\n",
    "        print(\"Epoch: {}/{}.. \".format(e+1, epochs),\n",
    "              \"Training Loss: {:.3f}.. \".format(running_loss/len(trainloader)),\n",
    "              \"Test Loss: {:.3f}.. \".format(test_loss/len(testloader)),\n",
    "              \"Test Accuracy: {:.3f}\".format(accuracy/len(testloader)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dropout**: Used to avoid overfitting\n",
    "\n",
    "`model.eval()`: Used to turn off dropout, set model in eval mode\n",
    "`model.train()`: Used to train on dropout, set model in train mode\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving & Loading\n",
    "\n",
    "1. The parameters are stored in `model.state_dict()` \n",
    "2. Saving: `torch.save(model.state_dict(), name)`\n",
    "3. Loading: `state_dict = torch.load(name)`\n",
    "4. Loading into model: `model.load_state_dict(state_dict)`\n",
    "\n",
    "However, with new model you get an error of incosistent tensors. To fix it we can:\n",
    "\n",
    "1. Create checkpoint variable with the input, output, and hidden sizes\n",
    "2. Note that we use `out_features` to get the layers in the hidden layers\n",
    "3. Save the checkpoint \n",
    "4. Load the checkpoint \n",
    "5. Change model to be of the same size \n",
    "6. Load the state_dict "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_checkpoint(filepath):\n",
    "    checkpoint = torch.load(filepath)\n",
    "    model = fc_model.Network(checkpoint['input_size'],\n",
    "                             checkpoint['output_size'],\n",
    "                             checkpoint['hidden_layers'])\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Image Data\n",
    "\n",
    "\n",
    "1. Transforms: Changes to the images such as resizing, cropping, converting to a tensor, etc..\n",
    "2. Data Loaders: Defining batch_size, setting shuffle to change order\n",
    "\n",
    "**Data Augmentation - Introducting randomness into data, including**\n",
    "2. `RandomRotation()` \n",
    "3. `RandomResizedCrop()` \n",
    "4. `RandomHorizontalFlip()`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'Cat_Dog_data'\n",
    "\n",
    "# TODO: Define transforms for the training data and testing data\n",
    "train_transforms = transforms.Compose([transforms.RandomRotation(30),\n",
    "                                       transforms.RandomResizedCrop(224),\n",
    "                                       transforms.RandomHorizontalFlip(),\n",
    "                                       transforms.ToTensor(),\n",
    "                                       transforms.Normalize([0.5, 0.5, 0.5],\n",
    "                                                           [0.5,0.5,0.5])])\n",
    "\n",
    "test_transforms = transforms.Compose([transforms.Resize(255),\n",
    "                                     transforms.CenterCrop(224),\n",
    "                                     transforms.ToTensor(),\n",
    "                                     transforms.Normalize([0.5, 0.5, 0.5],\n",
    "                                                         [0.5, 0.5, 0.5])])\n",
    "\n",
    "\n",
    "# Pass transforms in here, then run the next cell to see how the transforms look\n",
    "train_data = datasets.ImageFolder(data_dir + '/train', transform=train_transforms)\n",
    "test_data = datasets.ImageFolder(data_dir + '/test', transform=test_transforms)\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(train_data, batch_size=32)\n",
    "testloader = torch.utils.data.DataLoader(test_data, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-trained Networks \n",
    "\n",
    "1. Convulational networks: Converting form of input in terms of 1s and 0s into something we understand\n",
    "2. `torchvision.models()`: Finding models \n",
    "3. Using a pretrained network for our own data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:anaconda3]",
   "language": "python",
   "name": "conda-env-anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
